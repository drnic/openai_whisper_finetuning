{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSt8Vot-xlPq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/ducanhdt/openai_whisper_finetuning.git \n",
        "%cd openai_whisper_finetuning/\n",
        "\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6oyWw46yS__"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import gdown\n",
        "url = \"https://drive.google.com/file/d/1Ljv_s7HrLR2nAK4kvA87EtYRIqK2MChT/view?usp=sharing\"\n",
        "output_file = \"vivos.tar.gz\"\n",
        "gdown.download(url,output_file, quiet=False,fuzzy=True)\n",
        "!tar -xvf vivos.tar.gz vivos\n",
        "%cd openai_whisper_finetuning/\n",
        "!mkdir data\n",
        "!mv vivos data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbR1Ziw3ziRd",
        "outputId": "f4825180-cffe-4cd8-a0b1-c015c1cc435e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘content’: File exists\n",
            "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 290MiB/s]\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | Whisper          | 71.8 M\n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "---------------------------------------------\n",
            "52.0 M    Trainable params\n",
            "19.8 M    Non-trainable params\n",
            "71.8 M    Total params\n",
            "287.304   Total estimated model params size (MB)\n",
            "Sanity Checking DataLoader 0:   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:99: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "Epoch 0:  93% 1440/1553 [12:36<00:59,  1.90it/s, loss=0.295, v_num=os01, train/loss=0.234]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  94% 1460/1553 [12:45<00:48,  1.91it/s, loss=0.295, v_num=os01, train/loss=0.234]\n",
            "Epoch 0:  95% 1480/1553 [12:54<00:38,  1.91it/s, loss=0.295, v_num=os01, train/loss=0.234]\n",
            "Epoch 0:  97% 1500/1553 [13:03<00:27,  1.91it/s, loss=0.295, v_num=os01, train/loss=0.234]\n",
            "Epoch 0:  98% 1520/1553 [13:12<00:17,  1.92it/s, loss=0.295, v_num=os01, train/loss=0.234]\n",
            "Epoch 0:  99% 1540/1553 [13:21<00:06,  1.92it/s, loss=0.295, v_num=os01, train/loss=0.234]\n",
            "Epoch 0: 100% 1553/1553 [13:27<00:00,  1.92it/s, loss=0.309, v_num=os01, train/loss=0.176, val/loss_step=0.352, val/cer_step=0.101, val/wer_step=0.333, val/loss_epoch=0.334, val/cer_epoch=0.127, val/wer_epoch=0.343]\n",
            "Epoch 1:  93% 1440/1553 [12:28<00:58,  1.92it/s, loss=0.198, v_num=os01, train/loss=0.177, val/loss_step=0.352, val/cer_step=0.101, val/wer_step=0.333, val/loss_epoch=0.334, val/cer_epoch=0.127, val/wer_epoch=0.343]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  94% 1460/1553 [12:37<00:48,  1.93it/s, loss=0.198, v_num=os01, train/loss=0.177, val/loss_step=0.352, val/cer_step=0.101, val/wer_step=0.333, val/loss_epoch=0.334, val/cer_epoch=0.127, val/wer_epoch=0.343]\n",
            "Epoch 1:  95% 1480/1553 [12:46<00:37,  1.93it/s, loss=0.198, v_num=os01, train/loss=0.177, val/loss_step=0.352, val/cer_step=0.101, val/wer_step=0.333, val/loss_epoch=0.334, val/cer_epoch=0.127, val/wer_epoch=0.343]\n",
            "Epoch 1:  97% 1500/1553 [12:55<00:27,  1.93it/s, loss=0.198, v_num=os01, train/loss=0.177, val/loss_step=0.352, val/cer_step=0.101, val/wer_step=0.333, val/loss_epoch=0.334, val/cer_epoch=0.127, val/wer_epoch=0.343]\n",
            "Epoch 1:  98% 1520/1553 [13:04<00:17,  1.94it/s, loss=0.198, v_num=os01, train/loss=0.177, val/loss_step=0.352, val/cer_step=0.101, val/wer_step=0.333, val/loss_epoch=0.334, val/cer_epoch=0.127, val/wer_epoch=0.343]\n",
            "Epoch 1:  99% 1540/1553 [13:12<00:06,  1.94it/s, loss=0.198, v_num=os01, train/loss=0.177, val/loss_step=0.352, val/cer_step=0.101, val/wer_step=0.333, val/loss_epoch=0.334, val/cer_epoch=0.127, val/wer_epoch=0.343]\n",
            "Epoch 1: 100% 1553/1553 [13:18<00:00,  1.94it/s, loss=0.214, v_num=os01, train/loss=0.124, val/loss_step=0.313, val/cer_step=0.108, val/wer_step=0.324, val/loss_epoch=0.303, val/cer_epoch=0.201, val/wer_epoch=0.331]\n",
            "Epoch 2:  93% 1440/1553 [12:18<00:57,  1.95it/s, loss=0.133, v_num=os01, train/loss=0.118, val/loss_step=0.313, val/cer_step=0.108, val/wer_step=0.324, val/loss_epoch=0.303, val/cer_epoch=0.201, val/wer_epoch=0.331] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  94% 1460/1553 [12:28<00:47,  1.95it/s, loss=0.133, v_num=os01, train/loss=0.118, val/loss_step=0.313, val/cer_step=0.108, val/wer_step=0.324, val/loss_epoch=0.303, val/cer_epoch=0.201, val/wer_epoch=0.331]\n",
            "Epoch 2:  95% 1480/1553 [12:36<00:37,  1.96it/s, loss=0.133, v_num=os01, train/loss=0.118, val/loss_step=0.313, val/cer_step=0.108, val/wer_step=0.324, val/loss_epoch=0.303, val/cer_epoch=0.201, val/wer_epoch=0.331]\n",
            "Epoch 2:  97% 1500/1553 [12:45<00:27,  1.96it/s, loss=0.133, v_num=os01, train/loss=0.118, val/loss_step=0.313, val/cer_step=0.108, val/wer_step=0.324, val/loss_epoch=0.303, val/cer_epoch=0.201, val/wer_epoch=0.331]\n",
            "Epoch 2:  98% 1520/1553 [12:53<00:16,  1.96it/s, loss=0.133, v_num=os01, train/loss=0.118, val/loss_step=0.313, val/cer_step=0.108, val/wer_step=0.324, val/loss_epoch=0.303, val/cer_epoch=0.201, val/wer_epoch=0.331]\n",
            "Epoch 2:  99% 1540/1553 [13:02<00:06,  1.97it/s, loss=0.133, v_num=os01, train/loss=0.118, val/loss_step=0.313, val/cer_step=0.108, val/wer_step=0.324, val/loss_epoch=0.303, val/cer_epoch=0.201, val/wer_epoch=0.331]\n",
            "Epoch 2: 100% 1553/1553 [13:08<00:00,  1.97it/s, loss=0.148, v_num=os01, train/loss=0.0685, val/loss_step=0.331, val/cer_step=0.271, val/wer_step=0.363, val/loss_epoch=0.296, val/cer_epoch=0.278, val/wer_epoch=0.329]\n",
            "Epoch 3:  93% 1440/1553 [12:34<00:59,  1.91it/s, loss=0.0798, v_num=os01, train/loss=0.0674, val/loss_step=0.331, val/cer_step=0.271, val/wer_step=0.363, val/loss_epoch=0.296, val/cer_epoch=0.278, val/wer_epoch=0.329]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  94% 1460/1553 [12:44<00:48,  1.91it/s, loss=0.0798, v_num=os01, train/loss=0.0674, val/loss_step=0.331, val/cer_step=0.271, val/wer_step=0.363, val/loss_epoch=0.296, val/cer_epoch=0.278, val/wer_epoch=0.329]\n",
            "Epoch 3:  95% 1480/1553 [12:53<00:38,  1.91it/s, loss=0.0798, v_num=os01, train/loss=0.0674, val/loss_step=0.331, val/cer_step=0.271, val/wer_step=0.363, val/loss_epoch=0.296, val/cer_epoch=0.278, val/wer_epoch=0.329]\n",
            "Epoch 3:  97% 1500/1553 [13:01<00:27,  1.92it/s, loss=0.0798, v_num=os01, train/loss=0.0674, val/loss_step=0.331, val/cer_step=0.271, val/wer_step=0.363, val/loss_epoch=0.296, val/cer_epoch=0.278, val/wer_epoch=0.329]\n",
            "Epoch 3:  98% 1520/1553 [13:10<00:17,  1.92it/s, loss=0.0798, v_num=os01, train/loss=0.0674, val/loss_step=0.331, val/cer_step=0.271, val/wer_step=0.363, val/loss_epoch=0.296, val/cer_epoch=0.278, val/wer_epoch=0.329]\n",
            "Epoch 3:  99% 1540/1553 [13:19<00:06,  1.93it/s, loss=0.0798, v_num=os01, train/loss=0.0674, val/loss_step=0.331, val/cer_step=0.271, val/wer_step=0.363, val/loss_epoch=0.296, val/cer_epoch=0.278, val/wer_epoch=0.329]\n",
            "Epoch 3: 100% 1553/1553 [13:25<00:00,  1.93it/s, loss=0.0915, v_num=os01, train/loss=0.0289, val/loss_step=0.289, val/cer_step=0.103, val/wer_step=0.314, val/loss_epoch=0.279, val/cer_epoch=0.142, val/wer_epoch=0.280]\n",
            "Epoch 4:  93% 1440/1553 [12:34<00:59,  1.91it/s, loss=0.0501, v_num=os01, train/loss=0.0464, val/loss_step=0.289, val/cer_step=0.103, val/wer_step=0.314, val/loss_epoch=0.279, val/cer_epoch=0.142, val/wer_epoch=0.280]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  94% 1460/1553 [12:44<00:48,  1.91it/s, loss=0.0501, v_num=os01, train/loss=0.0464, val/loss_step=0.289, val/cer_step=0.103, val/wer_step=0.314, val/loss_epoch=0.279, val/cer_epoch=0.142, val/wer_epoch=0.280]\n",
            "Epoch 4:  95% 1480/1553 [12:52<00:38,  1.92it/s, loss=0.0501, v_num=os01, train/loss=0.0464, val/loss_step=0.289, val/cer_step=0.103, val/wer_step=0.314, val/loss_epoch=0.279, val/cer_epoch=0.142, val/wer_epoch=0.280]\n",
            "Epoch 4:  97% 1500/1553 [13:01<00:27,  1.92it/s, loss=0.0501, v_num=os01, train/loss=0.0464, val/loss_step=0.289, val/cer_step=0.103, val/wer_step=0.314, val/loss_epoch=0.279, val/cer_epoch=0.142, val/wer_epoch=0.280]\n",
            "Epoch 4:  98% 1520/1553 [13:10<00:17,  1.92it/s, loss=0.0501, v_num=os01, train/loss=0.0464, val/loss_step=0.289, val/cer_step=0.103, val/wer_step=0.314, val/loss_epoch=0.279, val/cer_epoch=0.142, val/wer_epoch=0.280]\n",
            "Epoch 4:  99% 1540/1553 [13:19<00:06,  1.93it/s, loss=0.0501, v_num=os01, train/loss=0.0464, val/loss_step=0.289, val/cer_step=0.103, val/wer_step=0.314, val/loss_epoch=0.279, val/cer_epoch=0.142, val/wer_epoch=0.280]\n",
            "Epoch 4: 100% 1553/1553 [13:25<00:00,  1.93it/s, loss=0.0598, v_num=os01, train/loss=0.0165, val/loss_step=0.284, val/cer_step=0.170, val/wer_step=0.255, val/loss_epoch=0.269, val/cer_epoch=0.234, val/wer_epoch=0.279]\n",
            "Epoch 4: 100% 1553/1553 [13:25<00:00,  1.93it/s, loss=0.0598, v_num=os01, train/loss=0.0165, val/loss_step=0.284, val/cer_step=0.170, val/wer_step=0.255, val/loss_epoch=0.269, val/cer_epoch=0.234, val/wer_epoch=0.279]INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 1553/1553 [13:29<00:00,  1.92it/s, loss=0.0598, v_num=os01, train/loss=0.0165, val/loss_step=0.284, val/cer_step=0.170, val/wer_step=0.255, val/loss_epoch=0.269, val/cer_epoch=0.234, val/wer_epoch=0.279]\n"
          ]
        }
      ],
      "source": [
        "!mkdir content\n",
        "!python finetune.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctJ1s_vhgokA",
        "outputId": "9b1e8ff3-e4d4-4cf8-cbb4-6ce48520d073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load checkpoint successfully from /content/openai_whisper_finetuning/content/artifacts/checkpoint/checkpoint-epoch=0004.ckpt\n",
            "Model is multilingual and has 71,825,920 parameters.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "cuda:0\n",
            "100% 95/95 [01:18<00:00,  1.21it/s]\n",
            "0                                     trả nên thụ động\n",
            "1                               củng khi tra họ dề dặt\n",
            "2        chị gạn hỏi anh thề sống thề chết là không có\n",
            "3    tiện thải lan nhân ta cũng phải nhất máy tốn k...\n",
            "4          cũng thổ lại gì đặt nhất trong hệ mạch trời\n",
            "5                    các vụ tham nhỏn và b bói kinh tế\n",
            "6     trống cứ cố lồi lại mặt dồ vướng phía gông trình\n",
            "7    vọt buôn dự án chị trợt bị ra cái dự án để kiế...\n",
            "8          đất cả mọi thứ đều kỳ lạ một các phí thường\n",
            "9    các chương trình ca nhạc phần lớn đều không độ...\n",
            "Name: hypothesis_clean, dtype: object\n",
            "___________\n",
            "0                                     trở nên thụ động\n",
            "1                             cũng khiến cho họ dè dặt\n",
            "2        chị gặn hỏi anh thề sống thề chết là không có\n",
            "3    điện thoại reng nhưng ta cũng phải nhấc máy đú...\n",
            "4       cũng thuộc loại dày đặc nhất trong hệ mặt trời\n",
            "5                  các vụ tham nhũng và bê bối kinh tế\n",
            "6      chúng cứ cố lùi lại mặc dù vướng víu gông xiềng\n",
            "7    bọn buôn dự án chạy chọt bày ra các dự án để k...\n",
            "8         tất cả mọi thứ đều kỳ lạ một cách phi thường\n",
            "9    các chương trình ca nhạc phần lớn đều không độ...\n",
            "Name: reference_clean, dtype: object\n",
            "WER: 24.27 %\n"
          ]
        }
      ],
      "source": [
        "!python evaluate_wer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW_8YZkrg7Go",
        "outputId": "ec455b24-59f0-4765-e178-0145a18dbd4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/openai_whisper_finetuning/content/artifacts/checkpoint/checkpoint-epoch=000'\n",
            "load checkpoint failt using origin weigth of base model\n",
            "Model is multilingual and has 71,825,920 parameters.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "cuda:0\n",
            "100% 95/95 [00:55<00:00,  1.73it/s]\n",
            "0                                    dạ nhan thấu đọng\n",
            "1                             cống khí trả hạ về việc.\n",
            "2       chị gặng hỏi anh thể xóm thể chết là không có.\n",
            "3    tuyện thái lên nhân ta cũng phải nhất mấy đống...\n",
            "4          cũng thộ lại dày đặt nhất trong hệ mà trời.\n",
            "5                 các vụ thăm nhỏng và bế bối kinh tế.\n",
            "6    chúng cứ có lùi lại mặt dù buống phí ước gong ...\n",
            "7    vọc vô dự án chạy chọn bày ra các dự án đẹp ki...\n",
            "8          đức cả mọi thứ đều kì là 1 cách phi thường.\n",
            "9    các chuyên trình ca nhạc phần lớn đều không độ...\n",
            "Name: hypothesis_clean, dtype: object\n",
            "___________\n",
            "0                                     trở nên thụ động\n",
            "1                             cũng khiến cho họ dè dặt\n",
            "2        chị gặn hỏi anh thề sống thề chết là không có\n",
            "3    điện thoại reng nhưng ta cũng phải nhấc máy đú...\n",
            "4       cũng thuộc loại dày đặc nhất trong hệ mặt trời\n",
            "5                  các vụ tham nhũng và bê bối kinh tế\n",
            "6      chúng cứ cố lùi lại mặc dù vướng víu gông xiềng\n",
            "7    bọn buôn dự án chạy chọt bày ra các dự án để k...\n",
            "8         tất cả mọi thứ đều kỳ lạ một cách phi thường\n",
            "9    các chương trình ca nhạc phần lớn đều không độ...\n",
            "Name: reference_clean, dtype: object\n",
            "WER: 45.56 %\n"
          ]
        }
      ],
      "source": [
        "!python evaluate_wer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks9N0TzyhZqE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('whisper')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "41365db0a9f7f1ca6e78616d07278a7e5ad038963b1dbcb92ce5b6eed7346761"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
